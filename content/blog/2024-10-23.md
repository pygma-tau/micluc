---
title: "Chinchilla Scaling Laws"
date: 2024-10-23
draft: true
---

Arguably,
$$
L \triangleq E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}
$$
is the most important equation of the decade, where $E$ is the underlying entropy of your dataset, $N$ is the number of parameters in your model, $D$ is the number of training tokens, and $L$ is the ultimate loss of your model. The original Chinchilla scaling paper fits the following values of $\alpha$ and $\beta$:
$$
L \triangleq E + \frac{A}{N^{0.34}} + \frac{B}{D^{0.28}},
$$
so optimal compute efficiency is reached when $N$ and $D$ are scaled roughly equally.

But what does this even mean? How do we quantify the number of parameters in a model? Do we care about dataset quality at all? And why is it so *clean*?

Well, it seems like Hoffmann et. al. may have done their regression wrong. Epoch's estimates of what the correct scaling law should be, based on the original 2022 data, are
$$
L \triangleq E + \frac{A}{N^{0.3478}} + \frac{B}{D^{0.3658}},
$$
which matches with the original heuristic recommendation of ~20 tokens per parameter (compared to the ~70 implied by the original scaling law).

The Deepseek team modifies the constraint under which to consider the scaling law, as well as what should be counted as a "model parameter". Hoffmann et. al. originally set $\text{FLOPs} \approx 6ND$, but Deepseek substitutes $N$ for a model scale parameter $M,$ measuring non-embedding FLOPs/token, such that their constraint is $C = MD.$ Specifically, they want to exclude parameters which contribute to vocab count, but also account for the computational overhead of attention. $M$ ends up being
$$
M = 72n_{\text{layer}}d^2_{\text{head}} + 12n_{\text{layer}}d_{\text{model}}l_{\text{seq}},
$$
and differs by up to ~50% from $N$ for small models. Deepseek also hypothesizes that the optimal model size / train token ratio increases as the dataset quality increases, because the optimal representation of a less noisy dataset is less complex. 
